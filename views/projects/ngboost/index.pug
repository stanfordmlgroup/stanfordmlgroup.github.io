extends ../../layout

block title
  title NGBoost: Natural Gradient Boosting for Probabilistic Prediction


block description
  meta(name='description', content='NGBoost: Natural Gradient Boosting for Probabilistic Prediction.')

block extralinks
  link(rel='stylesheet', type='text/css', href='/projects/chexnet/css/chexnet.css')

block extrascripts

block content
  section#header
    .container
      .row
        .col-lg-12
          h1 NGBoost: Natural Gradient Boosting for Probabilistic Prediction
          p Tony Duan*, Anand Avati*, Daisy Yi Ding, Sanjay Basu, Andrew Ng, Alejandro Schuler

  section
    .container
      .row
        h2 Predictive Uncertainty Estimation in the real world.
        .col-lg-6
          p
          p Estimating the uncertainty in the predictions of a machine learning model is crucial for production deployments in the real world. Not only do we want our models to make accurate predictions, but we also want a correct estimate of uncertainty along with each prediction. When model predictions are part of an automated decision-making workflow or production line, predictive uncertainty estimates are important for determining manual fallback alternatives or for human inspection and intervenion.
          p
          p Probabilistic prediction (or probabilistic forecasting), which is the approach where the model outputs a full probability distribution over the entire outcome space, is a natural way to quantify those uncertainties.
          p
          p Compare the point predictions vs probabilistic predictions in the following examples.
          p
        .col-lg-6
           table.ui.celled.table.center.aligned
            thead
              tr
               th Question
               th Point Prediction (No uncertainty estimate)
               th Probabilistic Prediction (Uncertainty is implicit)
            tbody
              tr
               td What will be the temperature at noon tomorrow?
               td 73.4 Fahrenheit
               td(align='left')
                 img(align='left', valign='top', src='/projects/ngboost/img/temp.png')
              tr
               td How long will this patient live?
               td 11.3 months
               td(align='left')
                 img(align='left', valign='top', src='/projects/ngboost/img/time.png')
            tfoot
              tr
               th(colspan=3)

  section
    .container
      .row
        h2 NGBoost brings predictive uncertainty estimation to Gradient Boosting.
        .col-lg-5
          img(src='/projects/ngboost/img/toy_single.png')
        .col-lg-7
          p
          |
          p Gradient Boosting methods have generally been among the top performers in predictive accuracy over structured or tabular input data.
          p NGBoost enables predictive uncertainty estimation with Gradient Boosting through probabilistic predictions (including real valued outputs). With the use of Natural Gradients, NGBoost overcomes technical challenges that make generic probabilistic prediction hard with gradient boosting.
          p We release an open source package of our implementation on GitHub.
          a.btn.btn-lg.btn-default(href="https://arxiv.org/abs/1910.03225") Read our paper
          |
          a.btn.btn-lg.btn-default(href="https://github.com/stanfordmlgroup/ngboost") GitHub
          |
          a.btn.btn-lg.btn-default(href="https://stanfordmlgroup.github.io/ngboost/intro.html") Documentation


  section
    .container
      .row
        h2 Simple and modular approach.
      .row
        .col-md-5
          p The NGBoost algorithm is simple to use. It has three abstract modular components that are chosen as configuration:
        .col-md-7
          img(src='/projects/ngboost/img/blocks.png')
      .row
        .col-md-12
          p <strong>Base Learner</strong>
          | The most common choice is Decision Trees, which tend to work well on structured inputs.
          p
          p <strong>Probability Distribution</strong>
          | The distribution needs to be compatible with the output type. For e.g. Normal distribution for real valued outputs, Bernoulli for binary outputs.
          p
          p <strong> Scoring rule </strong>
          |  Maximum Likelihood Estimation is an obvious choice. More robust rules such as Continuous Ranked Probability Score are also suitable.
          p
          p The above choices can be mixed and matched to be customized for the specific prediction problem at hand.


  section
    .container
      .row
        h2 The natural gradient makes learning efficient and effective.
      .row
        .col-md-4
          img(src='/projects/ngboost/img/toy_naive.gif')
        .col-md-4
          img(src='/projects/ngboost/img/toy_natural.gif')
        .col-md-4
          p
          p
          p Our key innovation is in employing the natural gradient to perform gradient boosting by casting it as a problem of determining the parameters of a probability distribution.
      .row
        .col-md-12
          p Ordinary gradients can be highly unsuitable for learning multi-parameter probability distributions (such as the Normal distribution). The training dynamics with the use of natural gradients tends to be much more stable and result in a better fit, as seen in the probabilistic regression example above.

  section
    .container
      .row
        .h2 Competitive performance in both uncertainty estimates and traditional metrics.
      .row
        .col-md-5
          p
          p NGBoost requires far less expertise to use than competing methods, and performs as well on common benchmarks. NGBoost has particularly strong performance on smaller data sets.
          a.btn.btn-lg.btn-default(href="https://arxiv.org/abs/1910.03225") Read our paper
          |
          a.btn.btn-lg.btn-default(href="https://github.com/stanfordmlgroup/ngboost") GitHub
        .col-md-7
          img(src='/projects/ngboost/img/results.png')

  section.bg-primary
    .container
      .row
        .col-md-12
          h3
            | If you have questions about our work,
            | contact us at:
          h4
            code avati@cs.stanford.edu
