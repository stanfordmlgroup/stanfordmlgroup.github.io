<!DOCTYPE html><!-- Author: Pranav Rajpurkar 2017-->
<html>

<head>
    <meta charset="utf-8">
    <title>CheXplanation Competition </title>
    <meta name="description"
        content="CheXplanation is a competition for x-ray pathology segmentation based on a new radiologist-annotated segmentation dataset.">
    <meta property="og:image" content="https://stanfordmlgroup.github.io/competitions/chexplanation/img/title.png">
    <meta property="og:title" content="CheXplanation: A Chest X-Ray Segmentation Dataset And Competition.">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://stanfordmlgroup.github.io/competitions/chexplanation/img/title.png">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <link rel="image_src" type="image/jpeg" href="/logo.jpg">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link href="/lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,600" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Muli:400,600" rel="stylesheet">
    <link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/lib/simple-line-icons/css/simple-line-icons.css">
    <link href="/css/theme.css" rel="stylesheet">
    <link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" type="text/css" href="/competitions/chexpert/css/index.css">
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <script src="/js/analytics.js"></script>
</head>

<body>
    <nav class="navbar navbar-default navbar-fixed-top" id="mainNav">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display-->
            <div class="navbar-header"><a class="navbar-brand page-scroll" href="/">Stanford ML Group</a></div>
            <!-- Collect the nav links, forms, and other content for toggling-->
        </div>
    </nav>
    <section id="header">
        <div class="container">
            <div class="row">
                <div class="col-lg-14"><img id="title-image" src="/competitions/chexplanation/img/CheXplanation.svg">
                    <h3 id="page-subtitle">A Chest X-Ray Segmentation Dataset And Competition
                    </h3>
                </div>
            </div>
        </div>
    </section>
    <section>
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <h1>What is CheXplanation?</h1>
                    <p>CheXplanation is a radiologist-annotated segmentation dataset on chest X-rays and
                        competition for automated pathology localization hosted by Stanford and VinBrain.
                    </p><a class="btn btn-default"
                        href="https://www.medrxiv.org/content/10.1101/2021.02.28.21252634v2"> Read the Paper (Saporta, Gui & Agrawal et
                        al.)</a>
                    <h2>Why CheXplanation?</h2>
                    <p> While deep learning has enabled automated medical imaging interpretation at a level shown to
                        surpass that of practicing
                        experts, the “black box” nature of neural networks represents a barrier to physicians’ trust and
                        model adoption in the clinical setting. Therefore, to encourage the development and validation
                        of more “interpretable” models for chest X-ray interpretation, we present a new
                        radiologist-annotated segmentation dataset and a competition for pathology localization.
                    </p>
                </div>
                <div class="col-md-5">
                    <h2>Leaderboard (coming soon)</h2>
                    <p>Will your model perform as well as radiologists in localizing different pathologies in chest
                        X-rays?</p>
                    <h3>How can I participate?</h3>
                    <p>CheXplanation uses a hidden test set with official evaluation of models. Teams submit
                        binary segmentation masks per pathology which will be compared against the reference
                        segmentations using mIoU (mean intersection over union). We also give teams the option to submit
                        confidence scores relating to each pixel for pointing game evaluation.
                    </p>
                    <p>Submission format and tutorial is in progress.</p>
                    <ul class="list-inline">
                        <li><a class="btn btn-lg btn-default"
                                >Submission
                                Tutorial (In Progress)</a></li>
                        <li></li>
                    </ul>
                </div>
            </div>
        </div>
    </section>
    <section class="gray">
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <h2>How did we produce the CheXplanation dataset?</h2>
                    <p>The dataset’s chest X-rays came from CheXpert, a large public dataset for chest X-ray
                        interpretation. We obtained pixel-level reference segmentations on the CheXpert validation and
                        test sets from two board-certified radiologists. For each chest X-ray, the radiologists were
                        asked to segment any of the following 10 conditions that were present in that chest X-ray, as
                        determined by CheXpert’s ground-truth labels: Airspace Opacity, Atelectasis, Cardiomegaly,
                        Consolidation, Edema, Enlarged Cardiomediastinum, Lung Lesion, Pleural Effusion, Pneumothorax,
                        and Support Devices.
                        We also established a human benchmark by collecting segmentations from three additional
                        radiologists. These radiologists were also asked to segment the 10 conditions of interest
                        present on each chest X-ray, as determined by CheXpert’s ground-truth labels.

                    </p>
                    <h2>What is our baseline model?</h2>
                    <p> Our baseline model leveraged saliency method, a weakly-supervised learning technique to generate
                        segmentations. For each chest X-ray, we used Grad-CAM to
                        generate heatmaps for each of the ten pathologies. We then applied a threshold to the heat maps
                        to produce binary segmentations in order to evaluate their overlap
                        with the radiologist reference segmentations. You may read the full details in the linked paper.

                    </p>
                </div>
                <div class="col-md-8"><img src="/competitions/chexplanation/img/pipeline.png">
                    <p> * Note: Only positive pathologies are shown in our example figure.
                    </p>
                </div>

            </div>
    </section>
    <section>
        <div class="container">
            <div class="row">
                <div class="col-md-8">
                    <h2>Validation and Test Sets</h2>
                    <p> The CheXpert validation set consists of 234 chest X-rays from 200 patients randomly
                        sampled from the full dataset and was labeled according to the consensus of three
                        board-certified radiologists. The test set consists of 668 chest X-rays from 500 patients not
                        included in the training or validation sets and was labeled according to the consensus of five
                        board-certified radiologists. </p>
                    <p>We provide the original X-ray images and reference segmentations of the validation set. The test
                        set will be used as a held-out set for leaderboard evaluation.</p>

                </div>
            </div>
        </div>
    </section>
    <section class="gray">
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <h1>Downloading the Dataset (v1.0)</h1>
                    <a class="btn btn-default"
                        href="https://stanfordaimi.azurewebsites.net/datasets/d6a91bb1-1d41-4ba7-9dab-51060c68a9b5"> Dataset Link</a>
                </div>
            </div>
        </div>
    </section>
    <section>
        <div class="container">
            <div class="row">
                <div class="col-md-7">
                    <h2>Benchmarking saliency methods for chest X-ray interpretation</h2>
                    <h3>Adriel Saporta *, Xiaotong Gui *, Ashwin Agrawal *, Anuj Pareek, Steven QH Truong, Chanh DT
                        Nguyen, Van-Doan Ngo, Jayne Seekins, Francis G Blankenberg, Andrew Ng, Matthew P Lungren, Pranav
                        Rajpurkar</h3>
                    <p>If you have questions about our work,
                        contact us at our <a href="https://groups.google.com/forum/#!forum/chexpert-dataset">google
                            group</a>.</p><a class="btn btn-lg btn-default"
                        href="https://www.medrxiv.org/content/10.1101/2021.02.28.21252634v2">Read
                        the Paper</a>
                </div>
            </div>
        </div>
    </section>
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-center"><a href="/"><img src="/img/stanfordmlgrouplogo.svg"></a></div>
            </div>
        </div>
    </footer>
    <script src="/lib/jquery/jquery.min.js"></script>
    <script src="/lib/bootstrap/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="/js/theme.js"></script>
</body>

</html>
