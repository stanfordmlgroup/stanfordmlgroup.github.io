<!DOCTYPE html><!-- Author: Pranav Rajpurkar 2017--><html><head><meta charset="utf-8"><title>CheXphoto Competition </title><meta name="description" content="CheXphoto is a competition for x-ray interpretation based on a new dataset of naturally and synthetically perturbed chest-xrays."><meta property="og:image" content="https://stanfordmlgroup.github.io/competitions/chexphoto/img/CheXphoto.png"><meta property="og:title" content="CheXphoto: A Dataset of Naturally and Synthetically Perturbed Chest X-Rays and Competition for Automated X-Ray Interpretation."><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://stanfordmlgroup.github.io/competitions/chexphoto/img/CheXphoto.png"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><link rel="image_src" type="image/jpeg" href="/logo.jpg"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link href="/lib/bootstrap/css/bootstrap.min.css" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Lato:400,600" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Muli:400,600" rel="stylesheet"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/lib/simple-line-icons/css/simple-line-icons.css"><link href="/css/theme.css" rel="stylesheet"><link href="//cdn-images.mailchimp.com/embedcode/classic-10_7.css" rel="stylesheet" type="text/css"><link rel="stylesheet" type="text/css" href="/competitions/chexpert/css/index.css"><script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script><script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script><script src="/js/analytics.js"></script></head><body><nav class="navbar navbar-default navbar-fixed-top" id="mainNav"><div class="container"><!-- Brand and toggle get grouped for better mobile display--><div class="navbar-header"><a class="navbar-brand page-scroll" href="/">Stanford ML Group</a></div><!-- Collect the nav links, forms, and other content for toggling--></div></nav><section id="header"><div class="container"><div class="row"><div class="col-lg-12"><img id="title-image" src="/competitions/chexphoto/img/CheXphoto.svg"><h3 id="page-subtitle">A Perturbed Chest X-Ray Dataset And Competition</h3></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7"><h1>What is CheXphoto?</h1><p>CheXphoto is a competition for x-ray interpretation based on a new dataset of naturally and synthetically perturbed chest x-rays hosted by Stanford and VinBrain.</p><a class="btn btn-default" href="https://arxiv.org/abs/2007.06199">Read the Paper (Phillips, Rajpurkar & Sabini et al.)</a><h2>Why CheXphoto?</h2><p> Chest radiography is the most common imaging examination globally, and is critical for screening, diagnosis, and management of many life threatening diseases. Most chest x-ray algorithms have been developed and validated on digital x-rays, while the vast majority of developing regions use films.
An appealing solution to scaled deployment is to leverage the ubiquity of smartphones for automated interpretation of film through cellphone photography. Automated interpretation of photos of chest x-rays at the same high-level of performance as with digital chest x-rays is challenging because photographs of x-rays introduce visual artifacts not commonly found in digital x-rays.
To encourage high model performance for this application,  we developed CheXphoto, a dataset of photos of chest x-rays and synthetic transformations designed to mimic the effects of photography.</p><p> With the launch of the CheXphoto competition, we are pleased to announce the release of an additional set of x-ray film images provided by VinBrain, a subsidiary of Vingroup. Please see <a href="#test_and_validation">Validation and Test Sets </a> for details.</p></div><div class="col-md-5"><h2>Leaderboard</h2><p> Will your model perform as well as radiologists in detecting different pathologies in chest X-rays?</p><p>We have launched as of August 18, 2021.</p><table class="table performanceTable"><tr><th>Rank</th><th>Date</th><th>Model</th><th>AUC Film</th><th>AUC Digital</th></tr><tr><td class="rank">1 <br></td><td><span class="date label label-default">Oct 01, 2021</span></td><td style="word-break:break-word;">LBC-v2 (ensemble)<em> Macao Polytechnic Institute </em></td><td><b>0.850</b></td><td>0.89</td></tr><tr><td class="rank">2 <br></td><td><span class="date label label-default">Sep 18, 2021</span></td><td style="word-break:break-word;">LBC-v0 (ensemble)<em> Macao Polytechnic Institute </em></td><td>0.820</td><td>0.89</td></tr><tr><td class="rank">3 <br></td><td><span class="date label label-default">Aug 26, 2021</span></td><td style="word-break:break-word;">Stellarium-CheXpert-Local (single model)<em> Macao Polytechnic Institute </em></td><td>0.802</td><td>0.88</td></tr><tr><td class="rank">4 <br></td><td><span class="date label label-default">May 07, 2021</span></td><td style="word-break:break-word;">MVD121<em> single model </em></td><td>0.762</td><td>0.83</td></tr><tr><td class="rank">5 <br></td><td><span class="date label label-default">May 11, 2021</span></td><td style="word-break:break-word;">MVD121-320<em> single model </em></td><td>0.758</td><td>0.84</td></tr><tr><td class="rank">6 <br></td><td><span class="date label label-default">Jun 22, 2020</span></td><td style="word-break:break-word;">ltts-mumbai</td><td>0.742</td><td>0.91</td></tr><tr><td class="rank">7 <br></td><td><span class="date label label-default">Aug 11, 2020</span></td><td style="word-break:break-word;">NewTrickTest (ensemble)<em> XBSJ </em></td><td>0.735</td><td>0.88</td></tr><tr><td class="rank">8 <br></td><td><span class="date label label-default">Jun 29, 2020</span></td><td style="word-break:break-word;">BASELINE Acorn<em> single model </em></td><td>0.732</td><td>0.85</td></tr><tr><td class="rank">9 <br></td><td><span class="date label label-default">Jun 22, 2020</span></td><td style="word-break:break-word;">CIO_Mumbai</td><td>0.722</td><td>0.91</td></tr><tr><td class="rank">10 <br></td><td><span class="date label label-default">Mar 04, 2021</span></td><td style="word-break:break-word;">mhealth_buet (single model)<em> BUET </em></td><td>0.710</td><td>0.87</td></tr><tr><td class="rank">11 <br></td><td><span class="date label label-default">Jul 14, 2020</span></td><td style="word-break:break-word;">CombinedTrainDenseNet121 (single model)<em> University of Westminster, Silva R. </em></td><td>0.707</td><td>0.86</td></tr><tr><td class="rank">12 <br></td><td><span class="date label label-default">Mar 28, 2021</span></td><td style="word-break:break-word;">Yoake (single model)<em> Macao Polytechnic Institute </em></td><td>0.682</td><td>0.86</td></tr><tr><td class="rank">13 <br></td><td><span class="date label label-default">Jan 12, 2021</span></td><td style="word-break:break-word;">mwowra-conditional (single)<em> AGH UST </em></td><td>0.646</td><td>0.80</td></tr><tr><td class="rank">14 <br></td><td><span class="date label label-default">Jun 11, 2021</span></td><td style="word-break:break-word;">AccidentNet V2 (single model)<em> Macao Polytechnic Institute </em></td><td>0.604</td><td>0.76</td></tr><tr><td class="rank">15 <br></td><td><span class="date label label-default">May 25, 2021</span></td><td style="word-break:break-word;">Stellarium (single model)<em> Macao Polytechnic Institute </em></td><td>0.599</td><td>0.71</td></tr><tr><td class="rank">16 <br></td><td><span class="date label label-default">Sep 25, 2020</span></td><td style="word-break:break-word;">Grp12BigCNN<em> ensemble </em></td><td>0.595</td><td>0.82</td></tr><tr><td class="rank">17 <br></td><td><span class="date label label-default">Sep 11, 2020</span></td><td style="word-break:break-word;">{koala-large} (single model)<em> SJTU </em></td><td>0.589</td><td>0.76</td></tr><tr><td class="rank">18 <br></td><td><span class="date label label-default">Aug 17, 2020</span></td><td style="word-break:break-word;">samg2003<a class="link" href="http://sambhavgupta.com">http://sambhavgupta.com</a></td><td>0.583</td><td>0.83</td></tr><tr><td class="rank">19 <br></td><td><span class="date label label-default">Jul 02, 2020</span></td><td style="word-break:break-word;">12ASLv2(single)<em> AITD </em></td><td>0.572</td><td>0.73</td></tr><tr><td class="rank">20 <br></td><td><span class="date label label-default">May 28, 2021</span></td><td style="word-break:break-word;">AccidentNet v1 (single model)<em> Macao Polytechnic Institute </em></td><td>0.561</td><td>0.74</td></tr></table><h3>How can I participate?</h3><p>CheXphoto measures model performance in relation to the CheXpert x-ray dataset. CheXphoto uses a hidden test set for official evaluation of models. Teams submit their executable code on Codalab, which is then run on a test set that is not publicly readable. Such a setup preserves the integrity of the test results.</p><p>Here's a tutorial walking you through official evaluation of your model. Once your model has been evaluated officially, your scores will be added to the leaderboard.</p><ul class="list-inline"><li><a class="btn btn-lg btn-default" href="https://worksheets.codalab.org/worksheets/0x693b0063ee504702b21f94ffb2d99c6d/">Submission Tutorial</a></li><li></li></ul></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-7"><h2>How did we produce the CheXphoto dataset?</h2><p>CheXphoto comprises a training set of natural photos and synthetic transformations of 10,507 x-rays from 3,000 unique patients that were sampled at random from the CheXpert training set, and a validation and test set of natural and synthetic transformations applied to all 234 x-rays from 200 patients and 668 x-rays from 500 patients in the CheXpert validation and test sets, respectively.</p><h3>Natural Transformations Dataset</h3><p>Natural photos consist of x-ray photography using cell phone cameras in various lighting conditions and environments. We developed two sets of natural photos: images captured through an automated process using a Nokia 6.1 cell phone, and images captured manually with an iPhone 8.</p><h3>Synthetic Transformations Dataset</h3><p>Synthetic transformations consist of automatic changes to the digital x-rays designed to make them look like photos of digital x-rays and x-ray films. We developed two sets of complementary synthetic transformations: digital transformations to alter contrast and brightness, and spatial transformations to add glare, moiré effects and perspective changes.
To ensure that the level of these transformations did not impact the quality of the image for physician diagnosis, the images were verified by a physician. In some cases, the effects may be visually imperceptible, but may still be adversarial for classification models. For both sets, we apply the transformations to the same set of 10,507 x-rays selected for the Nokia10k dataset.</p><a class="btn btn-lg btn-default" href="https://github.com/stanfordmlgroup/cheXphoto">View on Github</a></div><div class="col-md-5"><img src="/competitions/chexphoto/img/distribution.png"><img src="/competitions/chexphoto/img/sample_images.png"></div></div></div></section><section id="test_and_validation"><div class="container"><div class="row"><div class="col-md-8"><h2>Validation and Test Sets</h2><p> We developed a CheXphoto validation and test set to be used for model validation and evaluation.
The validation set comprises natural photos and synthetic transformations of all 234 x-rays in the CheXpert validation set, and is included in the public release, while the test set comprises natural photos of all 668 x-rays in the CheXpert test set, and is withheld for evaluation purposes.</p><p>We generated the natural photos of the validation set by manually capturing images of x-rays displayed on a 2560×1080 monitor using a OnePlus 6 cell phone, following a protocol that mirrored the iPhone1k dataset. Synthetic transformations of the validation images were produced using the same protocol as the synthetic training set.
The test set was captured using an iPhone 8, following the same protocol as the iPhone1k dataset.</p><p>The validation set contains an additional 250 cell phone images of chest x-ray films provided by VinBrain, a subsidiary of Vingroup in Vietnam. These films were originally collected by VinBrain through joint research projects with leading lung hospitals in Vietnam. An additional 250 film images have been withheld in the test set for model evaluation.</p></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-7"><h1>Downloading the Dataset (v1.0)</h1><p>Please read the Stanford University School of Medicine CheXphoto Dataset Research Use Agreement. Once you register to download the CheXphoto dataset, you will receive a link to the download over email. Note that you may not share the link to download the dataset with others.</p><div class="well" id="agreement"><h3>Stanford University School of Medicine CheXplanation Dataset Research Use Agreement</h3>
<p>By registering for downloads from the CheXphoto Dataset, you are agreeing to this Research Use Agreement, as well as to the Terms of Use of the Stanford University School of Medicine website as posted and updated periodically at http://www.stanford.edu/site/terms/.</p>
<p>1. Permission is granted to view and use the CheXphoto Dataset without charge for personal, non-commercial research purposes only. Any commercial use, sale, or other monetization is prohibited.</p>
<p>2. Other than the rights granted herein, the Stanford University School of Medicine (&ldquo;School of Medicine&rdquo;) retains all rights, title, and interest in the CheXphoto Dataset.</p>
<p>3. You may make a verbatim copy of the CheXphoto Dataset for personal, non-commercial research use as permitted in this Research Use Agreement. If another user within your organization wishes to use the CheXplanation Dataset, they must register as an individual user and comply with all the terms of this Research Use Agreement.</p>
<p>4. YOU MAY NOT DISTRIBUTE, PUBLISH, OR REPRODUCE A COPY of any portion or all of the CheXplanation Dataset to others without specific prior written permission from the School of Medicine.</p>
<p>5. YOU MAY NOT SHARE THE DOWNLOAD LINK to the CheXplanation dataset to others. If another user within your organization wishes to use the CheXplanation Dataset, they must register as an individual user and comply with all the terms of this Research Use Agreement.</p>
<p>6. You must not modify, reverse engineer, decompile, or create derivative works from the CheXplanation Dataset. You must not remove or alter any copyright or other proprietary notices in the CheXplanation Dataset.</p>
<p>7. The CheXplanation Dataset has not been reviewed or approved by the Food and Drug Administration, and is for non-clinical, Research Use Only. In no event shall data or images generated through the use of the CheXplanation Dataset be used or relied upon in the diagnosis or provision of patient care.</p>
<p>8. THE CheXplanation DATASET IS PROVIDED "AS IS," AND STANFORD UNIVERSITY AND ITS COLLABORATORS DO NOT MAKE ANY WARRANTY, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE, NOR DO THEY ASSUME ANY LIABILITY OR RESPONSIBILITY FOR THE USE OF THIS CheXplanation DATASET.</p>
<p>9. You will not make any attempt to re-identify any of the individual data subjects. Re-identification of individuals is strictly prohibited. Any re-identification of any individual data subject shall be immediately reported to the School of Medicine.</p>
<p>10. Any violation of this Research Use Agreement or other impermissible use shall be grounds for immediate termination of use of this CheXplanation Dataset. In the event that the School of Medicine determines that the recipient has violated this Research Use Agreement or other impermissible use has been made, the School of Medicine may direct that the undersigned data recipient immediately return all copies of the CheXplanation Dataset and retain no copies thereof even if you did not cause the violation or impermissible use.</p>
<p>In consideration for your agreement to the terms and conditions contained here, Stanford grants you permission to view and use the CheXplanation Dataset for personal, non-commercial research. You may not otherwise copy, reproduce, retransmit, distribute, publish, commercially exploit or otherwise transfer any material.</p>
<h4>Limitation of Use</h4>
<p>You may use CheXplanation Dataset for legal purposes only.</p>
<p>You agree to indemnify and hold Stanford harmless from any claims, losses or damages, including legal fees, arising out of or resulting from your use of the CheXplanation Dataset or your violation or role in violation of these Terms. You agree to fully cooperate in Stanford&rsquo;s defense against any such claims. These Terms shall be governed by and interpreted in accordance with the laws of California. </p></div><!-- Begin Mailchimp Signup Form -->
<div id="mc_embed_signup">
<form action="https://stanford.us10.list-manage.com/subscribe/post?u=d5a3df4e344026a19f5c0b760&amp;id=cdb7e3b4a7" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
<div id="mc_embed_signup_scroll">
<div class="indicates-required"><span class="asterisk">*</span> indicates required</div>
<div class="mc-field-group">
<label for="mce-EMAIL">Email Address<span class="asterisk">*</span>
</label>
<input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL">
</div>
<div class="mc-field-group">
<label for="mce-FNAME">First Name  <span class="asterisk">*</span>
</label>
<input type="text" value="" name="FNAME" class="required" id="mce-FNAME">
</div>
<div class="mc-field-group">
<label for="mce-LNAME">Last Name  <span class="asterisk">*</span>
</label>
<input type="text" value="" name="LNAME" class="required" id="mce-LNAME">
</div>
<div class="mc-field-group size1of2">
<label for="mce-PHONE">Phone Number  <span class="asterisk">*</span>
</label>
<input type="text" name="PHONE" class="required" value="" id="mce-PHONE">
</div>
<div class="mc-field-group">
<label for="mce-SCHOOL">School / Organization  <span class="asterisk">*</span>
</label>
<input type="text" value="" name="SCHOOL" class="required" id="mce-SCHOOL">
</div>
<div class="mc-field-group">
<label for="mce-ROLE">Role / Title  <span class="asterisk">*</span>
</label>
<input type="text" value="" name="ROLE" class="required" id="mce-ROLE">
</div>
<div id="mce-responses" class="clear">
<div class="response" id="mce-error-response" style="display:none"></div>
<div class="response" id="mce-success-response" style="display:none"></div>
</div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
<div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_d5a3df4e344026a19f5c0b760_cdb7e3b4a7" tabindex="-1" value=""></div>
<div class="clear"><input type="submit" value="Register" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
</div>
</form>
</div>
<script type='text/javascript' src='//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js'></script><script type='text/javascript'>(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';fnames[4]='PHONE';ftypes[4]='phone';fnames[3]='SCHOOL';ftypes[3]='text';fnames[5]='ROLE';ftypes[5]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script>
<!--End mc_embed_signup--></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7"><h2>CheXphoto: 10,000+ Smartphone Photos and Synthetic Photographic Transformations of Chest X-rays for Benchmarking Deep Learning Robustness</h2><h3>Nick A. Phillips *, Pranav Rajpurkar *, Mark Sabini *, Rayan Krishnan, Sharen Zhou, Anuj Pareek, Nguyet Minh Phu, Chris Wang, Mudit Jain, Nguyen Duong Du, Steven QH Truong, Andrew Y. Ng, and Matthew P. Lungren</h3><p>If you have questions about our work,
contact us at our <a href="https://groups.google.com/forum/#!forum/chexpert-dataset">google group</a>.</p><a class="btn btn-lg btn-default" href="https://arxiv.org/abs/2007.06199">Read the Paper</a></div></div></div></section><footer><div class="container"><div class="row"><div class="col-md-12 text-center"><a href="/"><img src="/img/stanfordmlgrouplogo.svg"></a></div></div></div></footer><script src="/lib/jquery/jquery.min.js"></script><script src="/lib/bootstrap/js/bootstrap.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script><script src="/js/theme.js"></script></body></html>