<!DOCTYPE html><!-- Author: Pranav Rajpurkar 2017--><html><head><meta charset="utf-8"><title>ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep Learning on Satellite Imagery</title><meta name="description" content="Classifying Deforestation Drivers in Satellite Imagery."><meta property="og:image" content="https://stanfordmlgroup.github.io/projects/forestnet/img/fig1.png"><meta name="twitter:image" content="https://stanfordmlgroup.github.io/projects/forestnet/img/fig1.png"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><link rel="image_src" type="image/jpeg" href="/logo.jpg"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link href="/lib/bootstrap/css/bootstrap.min.css" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Lato:400,600" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Muli:400,600" rel="stylesheet"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/lib/simple-line-icons/css/simple-line-icons.css"><link href="/css/theme.css" rel="stylesheet"><link rel="stylesheet" type="text/css" href="/projects/chexnext/css/chexnext.css"><script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script><script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script><script src="/js/analytics.js"></script></head><body><nav class="navbar navbar-default navbar-fixed-top" id="mainNav"><div class="container"><!-- Brand and toggle get grouped for better mobile display--><div class="navbar-header"><a class="navbar-brand page-scroll" href="/">Stanford ML Group</a></div><!-- Collect the nav links, forms, and other content for toggling--></div></nav><section id="header"><div class="container"><div class="row"><div class="col-lg-10"><h1>ForestNet: Classifying Drivers of Deforestation in Indonesia using Deep Learning on Satellite Imagery</h1><p>Jeremy Irvin *, Hao Sheng *, Neel Ramachandran, Sonja Johnson-Yu, Sharon Zhou, Rose Rustowicz, Kyle Story, Cooper Elsworth, Kemen Austin<sup>&#8224;</sup>, Andrew Y. Ng<sup>&#8224;</sup></p></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7"><h2>We developed a deep learning model called ForestNet to classify the drivers of deforestation using satellite imagery.</h2><p>Characterizing the processes leading to deforestation is critical to the development and implementation of targeted forest conservation and management policies. Methods to automate forest loss driver classification enable spatially-broad and temporally-dense driver attribution with significant implications on forest conservation policies.</p><a class="btn btn-lg btn-default" href="https://arxiv.org/abs/2011.05479">Read The Paper (Irvin & Sheng et al.)</a></div><div class="col-md-5"><img src="/projects/forestnet/img/fig0.gif" style="max-height:300px;"></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-6"><img src="/projects/forestnet/img/fig_a1.png"></div><div class="col-md-6"><h2>We curated a dataset of satellite imagery with expert forest loss driver annotations.</h2><p>The dataset consists of 2,756 satellite images of forest loss events with driver annotations. Global Forest Change (GFC) published maps were used to obtain forest loss events, each represented as a polygon and associated with a year indicating when the forest loss event occurred. An expert interpreter annotated each event with the direct driver of deforestation using high resolution satellite imagery from Google Earth. The driver annotations were grouped into Plantation, Smallholder Agriculture, Grassland/shrubland, and Other.</p><p> We captured each forest loss region with Landsat 8 satellite imagery acquired within five years of the eventâ€™s occurrence using a custom cloud-minimizing search procedure. Using this procedure, we obtained exactly one composite image for each example and additional images for any individual cloud-filtered scenes. Imagery was processed and downloaded using the <a href="https://www.descarteslabs.com/" target="_blank">Descartes Labs platform</a>. The dataset can be downloaded below (<a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 License</a>).</p><a class="btn btn-lg btn-default" href="http://download.cs.stanford.edu/deep/ForestNetDataset.zip">Download the Dataset</a></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-12"><img src="/projects/forestnet/img/fig1.png" style="max-height:700px;"></div><div class="col-md-12"><h2>ForestNet uses scene data augmentation, transfer learning, and multi-modal fusion.</h2><p>We trained deep learning models on the dataset to classify satellite imagery of forest loss events. Instead of a canonical multi-class classification approach, we formulate the task as semantic segmentation to (1) address that there are often multiple land uses within a single image, (2) implicitly utilize the information specific to the loss region, and (3) allow for high resolution (15m) predictions that can be used to predict different drivers for multiple loss regions of varying sizes. At test-time, the per-pixel model predictions in the forest loss region are used to obtain a single classification of the whole region.</p><p>We investigated the effect of (a) using scene data augmentation (SDA) where we randomly sample from the scenes and composite images during training to capture changes in landscape over time, (b) pre-training (PT) the model on a large land cover dataset in Indonesia that we curated, and (c) using multi-modal fusion with a variety of auxiliary predictors.</p></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-6"><h2>ForestNet achieves higher classification performance than standard driver classification models.</h2><p>Following recent work on automatically classifying land-use conversion from deforestation, we developed random forest (RF) models that input various variables, including topographic, climatic, soil, accessibility, proximity, and spectral imaging predictors. We found that all of the CNN models outperformed the RF models on the validation set.</p><p>The best performing model, which we call ForestNet, used a Feature Pyramid Network architecture with an EfficientNet-B2 backbone. The use of SDA provided large performance gains on the validation set, and land cover pre-training and incorporating auxiliary predictors each led to additional performance improvements.</p></div><div class="col-md-6"><h2> </h2><table class="table performanceTable"><thead> </thead><tr><th rowspan="2">Model </th><th rowspan="2">Predictors </th><th colspan="2" style="text-align:center;">Val</th><th colspan="2" style="text-align:center;">Test</th></tr><tr><th style="text-align:center;">Acc</th><th style="text-align:center;">F1</th><th style="text-align:center;">Acc</th><th style="text-align:center;">F1</th></tr><tbody></tbody><tr><td>RF</td><td>Visible</td><td style="text-align:center;">0.56</td><td style="text-align:center;">0.49</td><td style="text-align:center;">0.49</td><td style="text-align:center;">0.44</td></tr><tr><td>RF </td><td>Visible + Aux</td><td style="text-align:center;">0.72</td><td style="text-align:center;">0.67</td><td style="text-align:center;">0.67</td><td style="text-align:center;">0.62</td></tr><tr><td>CNN </td><td>Visible</td><td style="text-align:center;">0.80</td><td style="text-align:center;">0.75</td><td style="text-align:center;">0.78</td><td style="text-align:center;">0.70</td></tr><tr><td>CNN + SDA </td><td>Visible</td><td style="text-align:center;">0.82</td><td style="text-align:center;">0.79</td><td style="text-align:center;">0.78</td><td style="text-align:center;">0.73</td></tr><tr><td>CNN + SDA + PT </td><td>Visible</td><td style="text-align:center;">0.83</td><td style="text-align:center;">0.80</td><td style="text-align:center;">0.80</td><td style="text-align:center;">0.74</td></tr><tr><td>CNN + SDA + PT </td><td>Visible + Aux</td><th style="text-align:center;">0.84</th><th style="text-align:center;">0.81</th><th style="text-align:center;">0.80</th><th style="text-align:center;">0.75</th></tr><p style="text-align:center;">Table 1. Accuracy and F1 scores on the validation and test sets. </p></table></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7"><h3>To learn more, read <a href="https://arxiv.org/abs/2011.05479">our publication </a>presented at the NeurIPS 2020 workshop on Tackling Climate Change with Machine Learning.</h3><div class="pad"></div><p>If you have questions about our work, contact us at:</p><h4><code>jirvin16@cs.stanford.edu</code> and <code>haosheng@cs.stanford.edu</code></h4></div></div></div></section><footer><div class="container"><div class="row"><div class="col-md-12 text-center"><a href="/"><img src="/img/stanfordmlgrouplogo.svg"></a></div></div></div></footer><script src="/lib/jquery/jquery.min.js"></script><script src="/lib/bootstrap/js/bootstrap.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script><script src="/js/theme.js"></script></body></html>