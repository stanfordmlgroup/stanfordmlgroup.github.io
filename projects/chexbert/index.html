<!DOCTYPE html><!-- Author: Pranav Rajpurkar 2017--><html><head><meta charset="utf-8"><title>CheXpedition: Investigating Generalization Challenges for Translation of Chest X-Ray Algorithms to the Clinical Setting</title><meta name="description" content="Investigating Generalization of Chest X-Ray Algorithms."><meta property="og:image" content="https://stanfordmlgroup.github.io/projects/chexpedition/img/fig1.png"><meta name="twitter:image" content="https://stanfordmlgroup.github.io/projects/chexpedition/img/fig1.png"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><link rel="image_src" type="image/jpeg" href="/logo.jpg"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link href="/lib/bootstrap/css/bootstrap.min.css" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Lato:400,600" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Muli:400,600" rel="stylesheet"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/lib/simple-line-icons/css/simple-line-icons.css"><link href="/css/theme.css" rel="stylesheet"><link rel="stylesheet" type="text/css" href="/projects/chexnext/css/chexnext.css"><script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script><script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script><script src="/js/analytics.js"></script></head><body><nav class="navbar navbar-default navbar-fixed-top" id="mainNav"><div class="container"><!-- Brand and toggle get grouped for better mobile display--><div class="navbar-header"><a class="navbar-brand page-scroll" href="/">Stanford ML Group</a></div><!-- Collect the nav links, forms, and other content for toggling--></div></nav><section id="header"><div class="container"><div class="row"><div class="col-lg-10"><h1>CheXpedition: Investigating Generalization Challenges for Translation of Chest X-Ray Algorithms to the Clinical Setting</h1><p>Pranav Rajpurkar *, Anirudh Joshi *, Anuj Pareek, Phil Chen, Amir Kiani, Jeremy Irvin, Andrew Y. Ng<sup>&#8224;</sup>, Matthew P. Lungren<sup>&#8224;</sup></p></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-6"><h2>We analysed the top CheXpert model submissions on their ability to generalize to challenges seen in real world deployment.</h2><p>Scaling automated chest radiography interpretation introduces novel challenges to deep learning  models such as out of distribution data, diseases not seen in training, practical deployment strategies that would work in any healthcare system.</p><a class="btn btn-lg btn-default" href="https://arxiv.org/pdf/2002.11379.pdf">Read Paper</a></div><div class="col-md-6"><img src="/projects/chexpedition/img/fig1.png" style="max-height:700px;"></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-6"><img src="/projects/chexpedition/img/fig2.png"></div><div class="col-md-6"><h2>CheXpert models generalize to the task of Tuberculosis detection; a task not seen during training.</h2><p>In global healthcare settings, it is likely for models to see diseases that weren't seen during training time. CheXpert models were evaluated on their generalizability performance on tuberculosis using consolidation labels as a proxy.</p><p> The average AUC of the models on two public TB test datasets (NIH Shenzhen and Montgomery were 0.815 and 0.893, competitive with results in literature when models are trained directly on those datasets.</p><p>We also found that the average performance of a model across tasks was a stronger predictor of performance on the tuberculosis dataset as compared to the performance of the model on any of the individual tasks. This suggests that training models to perform well across tasks may allow them to perform better on unseen images than models that optimize for a single task.</p></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7 col-md-push-5"><img src="/projects/chexpedition/img/fig3_new.png"></div><div class="col-md-5 col-md-pull-7"><h2>CheXpert models maintain high performance on photos of chest radiographs.</h2><p>Scaled deployment demands a solution that can navigate an endless array of medical imaging and IT infrastructures. Leveraging smartphones as the tool for automated chest radiograph analysis allows for maximal accessibility.
While prior literature indicated poor generalizability of deep learning models on photos of images, CheXpert trained models achieved a mean AUC of 0.916 on photos of the CheXpert test set, compared with an AUC of 0.924 on the original CheXpert test set. All of the models had mean AUCs higher than 0.9, and were within 0.01 AUC of their performance on the original images. </p><p>Using photos of chest x-rays as input into chest-xray algorithms could enable any physician with a smartphone to get instant AI algorithm assistance.</p></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-6"><img src="/projects/chexpedition/img/fig4_new.png" style="max-height:900px;"></div><div class="col-md-6"><h2>CheXpert models generalize to data from an external institution </h2><p> We evaluated the performance of the top CheXpert models on a dataset from an external institution (NIH). Chest x-ray algorithms developed from data from one institution have not shown sustained performance when externally validated on data from a different unrelated institution. This is critical for safe deployment of these algorithms across healthcare systems.</p><p>The models achieved an average performance of 0.897 AUC across the 5 CheXpert competition tasks on the test set from the external institution. On Atelectasis, Cardiomegaly, Edema, and Pleural Effusion, the mean sensitivities of the models of 0.750, 0.617, 0.712, and 0.806 respectively, are higher than the mean radiologist sensitivities of 0.646, 0.485, 0.710, and 0.761 (at the mean radiologist specificities of 0.806, 0.924, 0.925, and 0.883 respectively). On Consolidation, the mean sensitivity of the models of 0.443 is lower than the mean radiologist sensitivity of 0.456 (at the mean radiologist specificity of 0.935).</p><a class="btn btn-lg btn-default" href="https://goo.gl/forms/KS4b2h37i0vTRLRN2">Partner with us</a></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7"><h3>To learn more, <a href="https://arxiv.org/pdf/2002.11379.pdf">read our publication</a>.</h3><a class="btn btn-lg btn-default" href="https://arxiv.org/pdf/2002.11379.pdf">Read Paper</a></div></div></div></section><footer><div class="container"><div class="row"><div class="col-md-12 text-center"><a href="/"><img src="/img/stanfordmlgrouplogo.svg"></a></div></div></div></footer><script src="/lib/jquery/jquery.min.js"></script><script src="/lib/bootstrap/js/bootstrap.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script><script src="/js/theme.js"></script></body></html>