<!DOCTYPE html><!-- Author: Pranav Rajpurkar 2017--><html><head><meta charset="utf-8"><title>CheXNeXt: Deep learning for chest radiograph diagnosis</title><meta name="description" content="Detecting Diseases from Chest X-Rays at the level of Radiologists."><meta property="og:image" content="https://stanfordmlgroup.github.io/projects/chexnext/img/chexnext-app.png"><meta name="twitter:image" content="https://stanfordmlgroup.github.io/projects/chexnext/img/chexnext-app.png"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><link rel="image_src" type="image/jpeg" href="/logo.jpg"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link href="/lib/bootstrap/css/bootstrap.min.css" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Lato:400,600" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Muli:400,600" rel="stylesheet"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/lib/simple-line-icons/css/simple-line-icons.css"><link href="/css/theme.css" rel="stylesheet"><link rel="stylesheet" type="text/css" href="/projects/chexnext/css/chexnext.css"><script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script><script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script><script src="/js/analytics.js"></script></head><body><nav class="navbar navbar-default navbar-fixed-top" id="mainNav"><div class="container"><!-- Brand and toggle get grouped for better mobile display--><div class="navbar-header"><a class="navbar-brand page-scroll" href="/">Stanford ML Group</a></div><!-- Collect the nav links, forms, and other content for toggling--></div></nav><section id="header"><div class="container"><div class="row"><div class="col-lg-10"><h1>CheXNeXt: Deep learning for chest radiograph diagnosis</h1><p>Pranav Rajpurkar *, Jeremy Irvin *, Robyn L. Ball, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis P. Langlotz, Bhavik N. Patel, Kristen W. Yeom, Katie Shpanskaya, Francis G. Blankenberg, Jayne Seekins, Timothy J. Amrhein, David A. Mong, Safwan S. Halabi, Evan J. Zucker, Andrew Y. Ng<sup>&#8224;</sup>, Matthew P. Lungren<sup>&#8224;</sup></p></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-6 col-md-push-6"><h2>We developed CheXNeXt, a deep learning algorithm to concurrently detect 14 clinically important diseases in chest radiographs.</h2><p>Chest radiograph interpretation is critical for the detection of acute thoracic diseases, including tuberculosis and lung cancer, which affect millions of people worldwide each year. This time-consuming task typically requires expert radiologists to read the images, leading to fatigue-based diagnostic error and lack of diagnostic expertise in areas of the world where radiologists are not available.</p></div><div class="col-sm-12 col-md-6 col-md-pull-6"><iframe width="100%" height="400px" src="https://www.youtube.com/embed/VJRCj-4E2iU?ecver=1&amp;rel=0&amp;modestbranding=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-6"><h2>CheXNeXt is trained to predict diseases on x-ray images and highlight parts of an image most indicative of each predicted disease.</h2><p>CheXNeXt is trained on the ChestX-ray14 dataset, one of the largest public repository of radiographs, containing 112,120 frontal-view chest radiographs of 30,805 unique patients. Each image in ChestX-ray14 was labeled using an automatic extraction method on radiology reports.</p><p>CheXNeXt's training process consists of 2 consecutive stages to account for the partially incorrect labels in the ChestX-ray14 dataset.
First, an ensemble of networks is trained on the training set to predict the probability that each of the 14 pathologies is present in the image.
The predictions of this ensemble are used to relabel the training and tuning sets.
A new ensemble of networks are finally trained on this relabeled training set.</p><p>Without any additional supervision, CheXNeXt produces heat maps that identify locations in the chest radiograph that contribute most to the network’s classification using class activation mappings (CAMs). </p></div><div class="col-md-6"><img src="/projects/chexnext/img/chexnext-cams.png" style="max-height:700px;"></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-6"><img src="/projects/chexnext/img/roc-compares.png"></div><div class="col-md-6"><h2>We compared CheXNeXt’s performance to the performance of 9 radiologists on a validation set.</h2><p>A validation set of 420 frontal-view chest radiographs was selected from ChestX-ray14 for radiologist annotation. The set was curated to contain at least 50 cases of each pathology according to the original labels provided in the dataset.</p><p> The majority vote of the annotations of 3 cardiothoracic specialist radiologists serves as the consensus reference standard for each image.
To compare to the algorithm, 6 board-certified radiologists from 3 academic institutions (average experience 12 years) and 3 senior radiology residents also annotated the validation set of 420 radiographs for all 14 labels.
The ROC curve of the algorithm is generated by varying the discrimination threshold (used to convert the output probabilities to binary predictions). The radiologist ROC curve is estimated by fitting an increasing concave curve to the radiologist operating points.</p><p>The algorithm achieved performance equivalent to the practicing radiologists on 10 pathologies, better on 1 pathology, and worse on 3 pathologies, on the AUC metric.</p></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7 col-md-push-5"><img src="/projects/chexnext/img/point-compares.png"></div><div class="col-md-5 col-md-pull-7"><h2>We found that CheXNeXt can diagnose certain pathologies in chest radiographs at a level comparable to practicing radiologists.</h2><p>CheXNeXt detected nodules with a sensitivity of 0.690 (95% CI 0.581–0.797) which was higher than the micro-average sensitivity of board-certified radiologists at 0.573 (95% CI 0.525–0.619). CheXNeXt maintained a high specificity of 0.900 (95% CI 0.867–0.931) in nodule detection compared with the radiologist score of 0.937 (95% CI 0.927–0.947).
The algorithm detected effusion with a specificity of 0.921 (95% CI 0.889–0.951), higher than micro-average board-certified radiologist specificity of 0.883 (95% CI 0.868–0.898) while achieving a sensitivity of 0.674 (95% CI 0.592–0.754), comparable to micro-average board-certified radiologist sensitivity of 0.761 (95% CI 0.731–0.790). </p><p>The average time for radiologists to complete labeling of 420 chest radiographs was 240 minutes (range 180–300 minutes). The deep learning algorithm labeled the same 420 chest radiographs in 1.5 minutes.</p></div></div></div></section><section class="gray"><div class="container"><div class="row"><div class="col-md-5 col-md-push-7"><img src="/projects/chexnext/img/chexnext-app.png"><img src="/projects/chexnext/img/chexnext-app2.png"></div><div class="col-md-7 col-md-pull-5"><h2>We are currently looking for research partnerships with healthcare providers that are interested in working with us to validate the technology.</h2><p> We hope that this technology may have the potential to improve healthcare delivery and increase access to chest radiograph expertise globally.
Towards this goal, the future of this research will depend on obtaining access to more sources of data for training and improving the model, as well as testing it on new populations and diagnoses. This additional data will help improve the accuracy and robustness of the model, making it more safe and effective. </p><a class="btn btn-lg btn-default" href="https://goo.gl/forms/KS4b2h37i0vTRLRN2">Partner with us</a></div></div></div></section><section><div class="container"><div class="row"><div class="col-md-7"><h3>To learn more, <a href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002686">read our publication</a> in PLOS Medicine.</h3><a class="btn btn-lg btn-default" href="https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002686">Read Paper</a></div></div></div></section><footer><div class="container"><div class="row"><div class="col-md-12 text-center"><a href="/"><img src="/img/stanfordmlgrouplogo.svg"></a></div></div></div></footer><script src="/lib/jquery/jquery.min.js"></script><script src="/lib/bootstrap/js/bootstrap.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script><script src="/js/theme.js"></script></body></html>