<!DOCTYPE html><!-- Author: Pranav Rajpurkar 2017--><html><head><meta charset="utf-8"><title>NGBoost: Natural Gradient Boosting for Probabilistic Prediction</title><meta name="description" content="NGBoost: Natural Gradient Boosting for Probabilistic Prediction."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><link rel="image_src" type="image/jpeg" href="/logo.jpg"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="icon" href="/favicon.ico" type="image/x-icon"><link href="/lib/bootstrap/css/bootstrap.min.css" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Lato:400,600" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Muli:400,600" rel="stylesheet"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="/lib/simple-line-icons/css/simple-line-icons.css"><link href="/css/theme.css" rel="stylesheet"><link rel="stylesheet" type="text/css" href="/projects/chexnet/css/chexnet.css"><script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script><script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script><script src="/js/analytics.js"></script></head><body><nav class="navbar navbar-default navbar-fixed-top" id="mainNav"><div class="container"><!-- Brand and toggle get grouped for better mobile display--><div class="navbar-header"><a class="navbar-brand page-scroll" href="/">Stanford ML Group</a></div><!-- Collect the nav links, forms, and other content for toggling--></div></nav><section id="header"><div class="container"><div class="row"><div class="col-lg-12"><h1>NGBoost: Natural Gradient Boosting for Probabilistic Prediction</h1><p>Tony Duan*, Anand Avati*, Daisy Yi Ding, Sanjay Basu, Andrew Ng, Alejandro Schuler</p></div></div></div></section><section><div class="container"><div class="row"><h2>Predictive Uncertainty Estimation in the real world.</h2><div class="col-lg-6"><p></p><p>Estimating the uncertainty in the predictions of a machine learning model is crucial for production deployments in the real world. Not only do we want our models to make accurate predictions, but we also want a correct estimate of uncertainty along with each prediction. When model predictions are part of an automated decision-making workflow or production line, predictive uncertainty estimates are important for determining manual fallback alternatives or for human inspection and intervenion.</p><p></p><p>Probabilistic prediction (or probabilistic forecasting), which is the approach where the model outputs a full probability distribution over the entire outcome space, is a natural way to quantify those uncertainties.</p><p></p><p>Compare the point predictions vs probabilistic predictions in the following examples.</p><p></p></div><div class="col-lg-6"><table class="ui celled table center aligned"><thead><tr><th>Question</th><th>Point Prediction (No uncertainty estimate)</th><th>Probabilistic Prediction (Uncertainty is implicit)</th></tr></thead><tbody><tr><td>What will be the temperature at noon tomorrow?</td><td>73.4 Fahrenheit</td><td align="left"><img align="left" valign="top" src="/projects/ngboost/img/temp.png"></td></tr><tr><td>How long will this patient live?</td><td>11.3 months</td><td align="left"><img align="left" valign="top" src="/projects/ngboost/img/time.png"></td></tr></tbody><tfoot><tr><th colspan="3"></th></tr></tfoot></table></div></div></div></section><section><div class="container"><div class="row"><h2>NGBoost brings predictive uncertainty estimation to Gradient Boosting.</h2><div class="col-lg-5"><img src="/projects/ngboost/img/toy_single.png"></div><div class="col-lg-7"><p></p><p>Gradient Boosting methods have generally been among the top performers in predictive accuracy over structured or tabular input data.</p><p>NGBoost enables predictive uncertainty estimation with Gradient Boosting through probabilistic predictions (including real valued outputs). With the use of Natural Gradients, NGBoost overcomes technical challenges that make generic probabilistic prediction hard with gradient boosting.</p><p>We release an open source package of our implementation on GitHub.</p><a class="btn btn-lg btn-default" href="https://arxiv.org/abs/1910.03225">Read our paper</a><a class="btn btn-lg btn-default" href="https://github.com/stanfordmlgroup/ngboost">GitHub</a><a class="btn btn-lg btn-default" href="https://stanfordmlgroup.github.io/ngboost/intro.html">Documentation</a></div></div></div></section><section><div class="container"><div class="row"><h2>Simple and modular approach.</h2></div><div class="row"><div class="col-md-5"><p>The NGBoost algorithm is simple to use. It has three abstract modular components that are chosen as configuration:</p></div><div class="col-md-7"><img src="/projects/ngboost/img/blocks.png"></div></div><div class="row"><div class="col-md-12"><p><strong>Base Learner</strong></p>The most common choice is Decision Trees, which tend to work well on structured inputs.<p></p><p><strong>Probability Distribution</strong></p>The distribution needs to be compatible with the output type. For e.g. Normal distribution for real valued outputs, Bernoulli for binary outputs.<p></p><p><strong> Scoring rule </strong></p> Maximum Likelihood Estimation is an obvious choice. More robust rules such as Continuous Ranked Probability Score are also suitable.<p></p><p>The above choices can be mixed and matched to be customized for the specific prediction problem at hand.</p></div></div></div></section><section><div class="container"><div class="row"><h2>The natural gradient makes learning efficient and effective.</h2></div><div class="row"><div class="col-md-4"><img src="/projects/ngboost/img/toy_naive.gif"></div><div class="col-md-4"><img src="/projects/ngboost/img/toy_natural.gif"></div><div class="col-md-4"><p></p><p></p><p>Our key innovation is in employing the natural gradient to perform gradient boosting by casting it as a problem of determining the parameters of a probability distribution.</p></div></div><div class="row"><div class="col-md-12"><p>Ordinary gradients can be highly unsuitable for learning multi-parameter probability distributions (such as the Normal distribution). The training dynamics with the use of natural gradients tends to be much more stable and result in a better fit, as seen in the probabilistic regression example above.</p></div></div></div></section><section><div class="container"><div class="row"><div class="h2">Competitive performance in both uncertainty estimates and traditional metrics.</div></div><div class="row"><div class="col-md-5"><p></p><p>NGBoost requires far less expertise to use than competing methods, and performs as well on common benchmarks. NGBoost has particularly strong performance on smaller data sets.</p><a class="btn btn-lg btn-default" href="https://arxiv.org/abs/1910.03225">Read our paper</a><a class="btn btn-lg btn-default" href="https://github.com/stanfordmlgroup/ngboost">GitHub</a></div><div class="col-md-7"><img src="/projects/ngboost/img/results.png"></div></div></div></section><section class="bg-primary"><div class="container"><div class="row"><div class="col-md-12"><h3>If you have questions about our work,
contact us at:</h3><h4><code>avati@cs.stanford.edu</code></h4></div></div></div></section><footer><div class="container"><div class="row"><div class="col-md-12 text-center"><a href="/"><img src="/img/stanfordmlgrouplogo.svg"></a></div></div></div></footer><script src="/lib/jquery/jquery.min.js"></script><script src="/lib/bootstrap/js/bootstrap.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script><script src="/js/theme.js"></script></body></html>